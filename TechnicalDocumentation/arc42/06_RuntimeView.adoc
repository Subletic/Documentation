:imagesdir: ./img
<<<

== Laufzeitsicht

Nachdem in der Bausteinsicht ein Überblick über die Bestandteile und deren Zusammenspiel im größeren Kontext gegeben wurde, konzentriert sich dieses Kapitel auf die kleineren Prozesse, um die Funktionsweisen bzw. Kommunikationswege zwischen den Komponenten oder Modulen zu verdeutlichen. Zunächst muss noch klar gestellt werden, dass das Frontend dem Backend unterstellt ist. Das bedeutet, dass das Frontend Einfluss auf das Backend hat, der zentrale Hauptprozess (das Generieren des Untertitels) jedoch im Backend stattfindet.

.Übersicht über die zentralen Prozesse
image::Process_Overview.drawio.svg[Static,80%,align="center"]

=== Kommunikation aus Sicht des Clients

.Protokoll und Ablauf mit welchem der Client mit dem Backend kommuniziert
image::Prozess_ClientCommunication.drawio.svg[Static,100%,align="center"]

Möchte sich ein _Client_ mit dem Backend verbinden, so kann er dies über den ClientExchangeController_ tun. _Subletic_ startet diesen nach dem Programmstart. Dazu wird ein WebSocket-Server verwendet. Der _Client_ kann mit dem Endpoint `/transcribe` die Verbindung aufbauen. Nachdem dieser erfolgt ist, wird vom _Client_ erwartet, dass dieser eine initiale Message schickt in dem das Untertitel-Format spezifiziert wird. Möglich sind `srt` oder `vtt`, welche als `message` übermittelt werden müssen. Passiert dies nicht, so wird die Verbindung geschlossen. Und wieder auf eine neue Verbindung gewartet. Wird ein gültiges Format übergeben, kann der _Client_ beginnen mit dem Senden von Audio-Paketen. Für jedes ankommende Paket wird die Untertitelerstellung angestoßen, und getrennt von diesem Prozess, bei Fertigstellung des Untertitels an den _Client_ zurück geschickt. Der Delay gibt dabei die Zeit an wie lange auf Bearbeitungen durch das Frontend gewartet wird, bis der Untertitel zurück geschickt wird. Wird die Verbindung vom _Client_ getrennt, bzw. kommen keine Pakete mehr an, so wird nach einem Timeout die Verbindung geschlossen und auf eine neue Verbindung gewartet. Falls der Abbruch vom _Client_ initiiert wurde, wird der restliche Buffer noch verarbeitet und der Untertitel zurück geschickt. Geht auf Seite des Backends etwas schief, so wird die Verbindung zum _Client_ geschlossen.

=== Konfigurations-Prozess

.Übersicht über den Konfigurations Prozess
image::Prozess_Konfiguration.drawio.svg[Static,align="center"]

Vor dem Start der eigentlichen Software kann die korrigierende Person zwei Parameter einstellen, zum einen den _Delay_ zwischen Ankunft einer Sprechblase bis zum Umwandeln und Senden des Untertitels. Der zweiten Parameter ist eine Liste von Wörtern, das _Soundslike-Dictionary_, welche _Speechmatics_ hilft schwierige Wörter zu erkennen. Beide Parameter werden in der Angular-Komponente _start-page_ vor Beginn der Korrektur eingegeben und an den _configuration.service_ übergeben. Der Button "_Weiter zum Live-Stream-Editor_" ruft die _continueToEditor()_-Methode auf die wiederum die Methode _postConfigurationToBackend()_ aufruft im _configuration.service_. Diese kümmert sich um das Senden der Konfiguration an das Backend. Dazu nutzt sie die _uploadConfiguration()_-Methode des _backend-provider.service_, welcher die Kommunikation mit dem Backend kapselt. Ein REST-Call sendet die _json_ welche die Konfiguration enthält an das Backend. Dort wird diese vom _ConfigurationController_ entgegengenommen und an den _ConfigurationService_ weitergeleitet und gespeichert. Nun kann sich der _BufferTimeMonitor_ bei Bedarf den _Delay_ holen. Verbindet sich ein neuer _Client_ mit dem _ClientExchangeController_, wird eine neuen _Speechmatics_-Verbindung aufgebaut. Vorher wird jedoch (falls vorhanden) das CustomDictionary angefordert. Dieses kann anschließend für den Aufruf im _SpeechmaticsSendService_ verwendet werden, um das Dictionary zu übergeben.

=== Publish-SpeechBubble-Prozess

.Prozessübersicht bei Erstellung einer neuen SpeechBubble
image::Prozess_SpeechBubble_Publish.drawio.svg[Static,100%,align="center"]

Die Erzeugung einer neuen _SpeechBubble_ beginnt damit, dass die _Speechmatics_ einen Teil der Transkription dem _ISpeechmaticsReceiverService_ übergibt. Diese wird mit der _HandleNewWord()_-Methode an den _WordProcessingService_ übergeben, der die Transkription in eine _SpeechBubble_ umwandelt. An zwei Stellen muss diese übergeben werden. Die Erste ist der _SpeechBubbleListService_, welche im Backend die derzeitigen _SpeechBubbles_ vorhält. Die zweite Stelle ist das Frontend, sodass der korrigierenden Person die neue _SpeechBubble_ angezeigt werden kann. Dazu wird der _IFrontendCommunicationService_ genutzt der die Kommunikation mit dem Frontend kapselt und die _PublishSpeechBubble()_-Methode anbietet, um neue Sprechblasen an das Frontend kommunizieren. Dazu wird intern der SignalR-Hub: _FrontendCommunicationHub_ genutzt, der Daten an das Frontend streamt, mithilfe der _newBubble_ Route. Im Frontend endet die Route beim _BackendListenerService_ der die neue _SpeechBubble_ vorhält. Zuvor hat das _textSheet_ im _backend-listener.service_ die vorgehaltenen _SpeechBubbles_ abonniert, sodass bei Ankunft neuer _SpeechBubbles_ diese direkt entgegengenommen und visualisieren werden.

=== Update-SpeechBubble-Prozess

.Prozessübersicht beim Updaten einer SpeechBubble
image::Prozess_SpeechBubble_Update.drawio.svg[Static,100%,align="center"]

Modifiziert der User eine Sprechblase ändert sich zwar direkt der interne _SpeechBubble_-Kontext (_TextSheet_) im Frontend, jedoch entsteht eine Inkonsistenz mit dem Backend. Um diese aufzulösen muss dem Backend die geänderte _SpeechBubble_ an das Backend kommuniziert werden. Dazu ruft das _textSheet_ den _backend-provider.service_ mit der _updateSpeechBubbles()_-Methode auf, welcher die Kommunikation mit dem Backend kapselt. Die Route _api/speechbubbles/update_ führt zu einer REST-API, welche durch den _SpeechBubbleController_ realisiert ist. Geänderte  _SpeechBubbles_ werden zum Schluss dem Backend-Kontext im _ISpeechBubbleListService_ bekannt gemacht, mithilfe der _ReplaceSpeechBubble()_-Methode.

=== Untertitel-Export-Prozess

.Prozessübersicht beim Exportieren der Untertitel
image::Prozess_Subtitle_Export.drawio.svg[Static,100%,align="center"]

Der Export der finalen Untertitel beginnt im _BufferTimeMonitor_, da hier festgestellt wird wenn eine _SpeechBubble_ veraltet ist und der Inhalt dem _Client_ als Untertitel ausgespielt werden soll. Drei Subprozesse müssen dazu angestoßen werden. Zunächst muss im Backend die _SpeechBubble_ aus der Liste herausgenommen werden, mit Hilfe der _DeleteOldestBubble()_-Methode im _ISpeechBubbleListService_. Mit der herausgenommenen _SpeechBubble_ wird nun der zweite Subprozess gestartet der die _SpeechBubble_ zu einem Untertitel exportiert und an den _Client_ weiterleitet. Dazu steht der _ISubtitleExporterService_ bereit, der mit der _ExportSubtitle()_-Methode die _SpeechBubble_ entgegennimmt und die Export-Logik kapselt. Intern nutzt der _ISubtitleExporterService_ den _ISubtitleConverter_, um eine _SpeechBubble_ in ein Untertitelformat zu konvertieren und anschließend mithilfe eines Streams diesen an den _Client_ weiterzuleiten. Der dritte und letzte Subprozess ist das Löschen der _SpeechBubble_ im Frontend. Dazu wird die _DeleteSpeechBubble()_-Methode im _IFrontendCommunicationService_ aufgerufen, analog zum Publish-Prozess. Hier wird jedoch nun die SignalR-Route: _deleteBubble_ verwendet. Im Frontend wird die betreffende _SpeechBubble_-Id über die entsprechende Route vom _backend-listener.service_ entgegengenommen. Das _textSheet_ kann nun die _SpeechBubble_ aus dem _TextSheet_ entfernen, da zuvor der Endpunkt abonniert wurde.

=== Audio-Handling-Prozess

.Prozessübersicht der Audio-Verarbeitung und Weiterleitung
image::Prozess_Audio.drawio.svg[Static,100%,align="center"]

Das Handeln des Audio-Streams beginnt, mit dem Senden von Audio-Paketen über einen WebSocket vom _Client_ zum _IAvReceiverService_. Nach Annahme der Audio-Daten werden diese über die _PushProcessedAudio()_-Methode an den _IAvProcessingService_ übergeben, um notwendige Vorverarbeitungen vorzunehmen. Die umgewandelten Audio-Daten werden anschießend an zwei Orte weiter geleitet. Der erste Ort ist _Speechmatics_, welche durch die Übertragung mittels WebSocket im _ISpeechmaticsSendService_ geschieht und mithilfe der _SendAudio()_-Methode angestoßen wird. Der zweite Ort ist das Frontend, welches durch die Übertragung mittels SignalR im _IFrontendCommunicationService_ angestoßen wird. Dazu wird mit den _Enqueue()_-Methode zunächst in einen Buffer geschrieben, welcher vom _FrontendCommunicationHub_ konsumiert wird, durch die _TryDequeue()_-Methode. Im Frontend abonniert der _audiohandler_ im _backend-listener.service_ den SignalR-Endpunkt "ReceiveAudioStream" und wird ab dann, mit neuen Audio-Daten versorgt, sobald diese im Backend verfügbar sind.